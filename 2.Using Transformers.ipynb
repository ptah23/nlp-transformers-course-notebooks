{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14f1400-bfbb-4f5a-82a7-1426a37e10f1",
   "metadata": {},
   "source": [
    "# Behind the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc06fdef-05b2-40de-8267-d9b7144ce41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998001456260681},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997509121894836}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "raw_inputs= [\"I've finally made it!\", \"Why is this not working!\"]\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087a7dc-27c2-4562-a9be-3121a785d446",
   "metadata": {},
   "source": [
    "## Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c1dd18-280d-47f8-8c28-1f0b7f355145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71f4a34-38f9-4a51-b95c-b3f49eeb8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2633, 2081, 2009,  999,  102],\n",
      "        [ 101, 2339, 2003, 2023, 2025, 2551,  999,  102,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d695d-4e29-4d8b-8fdd-835454414816",
   "metadata": {},
   "source": [
    "## Going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf54c54d-7a91-4bf2-974e-d7e235f49b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47f441-f63c-45ea-8bcc-439be95b82d1",
   "metadata": {},
   "source": [
    "### A high-dimensional vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afc893f5-3044-4ff4-9905-8b062297c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a91f95-9cdd-4514-bf1c-d7074a01c65c",
   "metadata": {},
   "source": [
    "### Model heads: Making sense out of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c719da-969b-4ca0-9505-304536d56810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe3881a-ba7a-409e-b7ad-18909875606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313556e4-0a51-4e02-ab3b-43b4a18e7124",
   "metadata": {},
   "source": [
    "## Postprocessing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cd3442f-3664-4f83-ab7c-b09592c8b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1149,  4.4030],\n",
      "        [ 4.6786, -3.6188]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff11f30f-d2b8-488e-9815-78d99b3bb812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9983e-04, 9.9980e-01],\n",
      "        [9.9975e-01, 2.4909e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838129bf-2e0b-40bf-8101-ad52822ab0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f3fca-a04a-4a47-a3b8-55269e8d2851",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d01b1c-c558-4fed-a720-07492aa4e56e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29ec8f61-1365-4abc-af42-bf494d3c8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "config = BertConfig()\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "249b854f-5be8-4275-9155-0ca5dd8686e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.21.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70157955-9507-4405-b02f-76a97a3c49a2",
   "metadata": {},
   "source": [
    "### Different loading methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ee40b28-c886-4343-831e-dc8aee66f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initialization\n",
    "from transformers import BertConfig, BertModel\n",
    "config = BertConfig()\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31305751-2a7c-4a74-8563-3cc205317e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcba40cb5b3e4957ad3063a0fb738f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f23aa76073542b2b19ce3794659eacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# initialize from pretrained\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35282f-0b06-44e7-bb9d-7b2e9c699087",
   "metadata": {},
   "source": [
    "### Saving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3c2c59f-fff4-4985-8685-b6f0ae56054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "012a1133-b8b2-4358-92d1-8465896d071d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls pretrained/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14548515-086e-4695-93ef-d8d67cadceb3",
   "metadata": {},
   "source": [
    "## Using a Transformer model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6d8c2ed-f974-4f54-ac69-8936691f6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool!\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d6bfc9-6b51-401c-827a-fbf69d2a134d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a921f02e86043139ffd7b22a2861e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478f90bb0bfe4380a980dc4f21f28349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a47bfb67124647be528f4b10178ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2560555-2f0c-4e74-aae7-63c22766fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8667, 106, 102], [101, 13297, 106, 102], [101, 8835, 106, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequences , padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20be43bd-38ba-4707-a14e-db2f3e0e75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f30d47-10de-435f-997f-308102fac8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f8445-29b6-4436-99d5-28df5ef8369b",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a986f-91e4-4138-8982-2f1070952e28",
   "metadata": {},
   "source": [
    "## Loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3168c4e8-49ff-45fb-af04-89dc1667dfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3baf827-45a6-42c2-b80e-db3bd7079a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5945a2b7c4364b1999473d374e933e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b80396-c804-42b8-981e-a4734890a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa60495-9002-44a2-98c0-e5312b0d996c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_pretrained/tokenizer_config.json',\n",
       " 'tokenizer_pretrained/special_tokens_map.json',\n",
       " 'tokenizer_pretrained/vocab.txt',\n",
       " 'tokenizer_pretrained/added_tokens.json',\n",
       " 'tokenizer_pretrained/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9501ce5-993d-4cff-b7ea-2914b724d2c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894bbb0-af5c-4f69-b118-db968f426498",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedd6003-c8c3-42c0-959b-94f3ea691463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Let's try to tokenize!\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7478028-41a6-42b1-9222-f6cd3b6415d7",
   "metadata": {},
   "source": [
    "### From tokens to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0209529-28dd-4ef7-a7c0-18b9c874f6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2421, 112, 188, 2222, 1106, 22559, 3708, 106]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88940541-365b-4673-bb6d-f6c55ff5e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'a',\n",
       " 'hugging',\n",
       " '##face',\n",
       " 'course',\n",
       " 'my',\n",
       " 'whole',\n",
       " 'life',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"I've been waiting for a huggingface course my whole life.\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefda0d5-4181-4ecf-92d7-3a429f380a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[146,\n",
       " 112,\n",
       " 1396,\n",
       " 1151,\n",
       " 2613,\n",
       " 1111,\n",
       " 170,\n",
       " 19558,\n",
       " 10931,\n",
       " 1736,\n",
       " 1139,\n",
       " 2006,\n",
       " 1297,\n",
       " 119]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd0668e-5a11-4fc8-b5bb-2d873248c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's try to tokenize!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0612da3-220b-4396-aba6-b67c23387937",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b6e276-8b8b-40e0-bc2a-446505ffaf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a Transformer network is simple'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([7993, 170, 13809, 23763, 2443, 1110, 3014])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f22921-be9d-4f06-a514-d3fbf4fa8d59",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e1903-59db-407f-b64a-5b36b24c0881",
   "metadata": {},
   "source": [
    "## Models expect a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10158e5-1e2e-4d72-ba03-0b0331f31678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e7e4aa340243b4a61bfc394d21be1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251bfdb3f1b94a859815c82db862a3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca632607aba2406992cc35da67b6fcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0c5fc1380841b1949ec3bc2724a724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08544dc-47be-4620-bf9d-0b0cc9c80a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sequence = \"I've been waiting for a Huggingface course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids]) # needed to add batching dimension\n",
    "\n",
    "output = model(input_ids)\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d224a7cf-0cbb-4a62-8518-b18480e063f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_ids = torch.tensor([ids, ids])\n",
    "output = model(batched_ids)\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadec9c-a97b-45e6-a027-ef83b3f3b986",
   "metadata": {},
   "source": [
    "## Padding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b192f327-3c65-4913-8abb-6e4a54061cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids =[\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69cfde71-263e-415f-89c8-8d386f857eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(sequence1_ids)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff381a35-5bb7-455b-a09c-25da10a1a856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(sequence2_ids)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e691129b-1d2d-4f38-8db9-23d5640ae731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5694, -1.3895],\n",
       "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(batched_ids)).logits # wrong result due to lack of attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dff03-bc38-4aa5-bf8b-b8c3f5a87499",
   "metadata": {},
   "source": [
    "## Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96822ba9-5a31-4922-ad96-31a387fcaa89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5694, -1.3895],\n",
       "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [\n",
    "    [1,1,1],\n",
    "    [1,1,0]\n",
    "]\n",
    "model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b369d799-fa5f-415d-a3f0-b67060521abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sentence1 = tokenizer.tokenize(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "sentence1_ids = tokenizer.convert_tokens_to_ids(sentence1)\n",
    "print(sentence1_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cc7c743-1988-475d-8a09-06e5df267627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 5223, 2023, 2061, 2172]\n"
     ]
    }
   ],
   "source": [
    "sentence2 = tokenizer.tokenize(\"I hate this so much\")\n",
    "sentence2_ids = tokenizer.convert_tokens_to_ids(sentence2)\n",
    "print(sentence2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "150636cc-1a8c-4d95-bdc1-b542b4ec5635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([sentence1_ids])).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2667284c-dc94-4182-9430-09459f69efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1744, -2.6848]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([sentence2_ids])).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f758a471-3e23-4e74-a554-ffdfdea038e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
    "    [1045, 5223, 2023, 2061, 2172, tokenizer.pad_token_id,tokenizer.pad_token_id,tokenizer.pad_token_id, tokenizer.pad_token_id,tokenizer.pad_token_id,tokenizer.pad_token_id, tokenizer.pad_token_id,tokenizer.pad_token_id,tokenizer.pad_token_id]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6865626-c3a7-4a01-a24c-1dd861ef5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = [\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,1,1,1,1,0,0,0,0,0,0,0,0,0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16c24da7-b3be-4c92-9798-b271dd31e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789],\n",
       "        [ 3.1744, -2.6848]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f1fe-2f96-45ad-b704-76581be055a3",
   "metadata": {},
   "source": [
    "## Longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b3080c1-270d-4b81-bdee-7abcfb9c4b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence # maximum number of tokens a single sentence can have (i.e. without special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eef2cdc-d545-49e8-86bb-f126c145772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # maximum number of tokens a model can handle (i.e. including special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc666b63-c12b-4fed-9e56-744ff44b0779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
