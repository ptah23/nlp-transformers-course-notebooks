{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1caf28-c3ea-4185-a049-a81505f6007a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d4aec7-70d0-4507-8ba4-4471d6ba7130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f53b4636e064c70b651238ab72432f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4665dfc7998b457d81f04d6adee6c712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4feed5c81e4d9488690cfb3f98acbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cd0661191b4c009edb1bcbe85f474a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bb26e2ce014e118af0bad215cf420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW,AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a769c8d1-88f8-4480-a2d8-50f903a7760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "batch[\"labels\"] = torch.tensor([1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fae8847-293f-42b4-8cee-43260f01dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492ca73-5586-4f42-a8bf-3c7c6a96311b",
   "metadata": {},
   "source": [
    "## Loading a dataset from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54110665-024c-4933-a00a-eb503edbce1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fcc76703a24104a848b333b26bedc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c50c054c86c4deea7298e10c948da85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765daef4b92143a092da6400d4271884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071b214c17384ca9878884536caa547d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c31064f6674d3bada833b980c114af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac9dd507b284358891c169d0a518d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de711c20ff654c029279f7cc23658baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ee024d-7e8a-4427-add5-d2f7bf9f99cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a653d5f4-c72a-4fe5-9a1e-fd2e9ac64de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689a2d4d-81f6-457a-beba-65fa6e6f1fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .',\n",
       " 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .',\n",
       " 'label': 0,\n",
       " 'idx': 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87330f5-ef13-4057-b207-52f2271f707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Tuition at four-year private colleges averaged $ 19,710 this year , up 6 percent from 2002 .',\n",
       " 'sentence2': 'For the current academic year , tuition at public colleges averaged $ 4,694 , up almost $ 600 from the year before .',\n",
       " 'label': 1,\n",
       " 'idx': 100}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106937-4531-4934-a2ea-65f330273866",
   "metadata": {},
   "source": [
    "## Preprocessing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b7d964-9d0d-46a7-aee8-87ed4cc9ba3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ddeb21-92dd-4cf8-b7c4-35e6d129aad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_train_dataset[14]['sentence1'], raw_train_dataset[14]['sentence2'])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62065b69-df26-4a0f-8cf5-b96fa4e47dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'said', 'the', 'coach', 'was', 'carrying', '38', 'passengers', '.', '[SEP]', 'the', 'head', 'of', 'the', 'local', 'disaster', 'unit', ',', 'g', '##yo', '##rgy', 'he', '##iz', '##ler', ',', 'said', 'the', 'coach', 'driver', 'had', 'failed', 'to', 'hee', '##d', 'red', 'stop', 'lights', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab1bd6b-eaa5-4076-a5a2-78ce257f55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7803c3a5-0015-441c-9668-f4e90e21c79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2513bc33a0a24e6f9c3bd8c0e527098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707fc7205c064195b74485717b15a4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1773363b4b8f4af2ac02c6fcc497a20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07b3b3-db38-417c-8f6f-ed32e803f489",
   "metadata": {},
   "source": [
    "## Dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c293ac-ea33-4ee8-bcc2-757348eeb508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddf461f0-7fbd-4de1-acaa-d4e0373c6aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "327639ab-bbc0-4029-931a-29ec6d498a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3fcba-13df-40e0-8cca-cb320c717589",
   "metadata": {},
   "source": [
    "## SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2b74014-7159-4ab8-b235-f86c7c541c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a722a1b998f14712b8e381828f128ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6a96d3aeaf48b792b9bfa8c692f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sst2_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aabcee6d-e015-482c-b4a6-8e14c565cf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10405e49-b86c-40b9-91d1-c211544e10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sst2_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42616604-d9c8-42a8-a449-19cf7785f0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07543aab2fa64d18ace9c511b9d30e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbfb68f84294f01bd12c2b75a549628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec4472c08764f67b94437200ab3335c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_sst2_datasets = sst2_datasets.map(tokenize_sst2_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "793c82e6-f6df-487e-875a-c9dd303a09a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 15, 10, 22, 13, 29, 6]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_samples = tokenized_sst2_datasets[\"train\"][:8]\n",
    "sst2_samples = {k: v for k, v in sst2_samples.items() if k not in [\"idx\", \"sentence\"]}\n",
    "[len(x) for x in sst2_samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "916a13dd-0469-4d0b-90f4-52fbfb296ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 29]),\n",
       " 'token_type_ids': torch.Size([8, 29]),\n",
       " 'attention_mask': torch.Size([8, 29]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_batch = data_collator(sst2_samples)\n",
    "{k: v.shape for k, v in sst2_batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f38cb-259a-4222-928d-8a476f655191",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb068b5-628b-46ee-995b-8a999c7b7ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b894daf00d44b4a6b23d7b6c8f0541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f600c6f65a45c4937934d679b4181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566eb465d1c24c648f5f2125505dae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a61b9892f3440ca8ea4df725b734082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecb95df5a5c487283ced3cb6f3ecc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517ef42dd3b34dc7b52337fa1881fa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399126cc49d44c9a90dcc86204e40bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125f2d2e9d11455698df1d676a2c823f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dccf3f1f5946ffbb8e4b9c4c535b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a51570cae8d45cb975b9aef5b0f9f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa28b7e0d5b54303906e69b750280262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b022e1-81fd-4291-9b11-da4330f432eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec96852c-5e3f-4f67-92f2-a97bc6fb9240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6d3ca899ed4f5dbc5ca7de4004f5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebadc0bc622435c9625ce9ef20c02ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21d9fa8b2e647dd9033593b152e1002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13161a95-9a81-4293-8886-c4bd72d55bc1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b46f97-3133-43ac-bbbd-0b47b11a11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4304fe-7056-4d80-bb7b-ced8885b9c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d701fbddb064015b9f07aa9c0d6a5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7955f7a6-8568-47b0-afe8-ccafacd5d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51eb9df-96ac-451a-bf32-db561529a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230811_223625-2izljwl4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ptah23/huggingface/runs/2izljwl4\" target=\"_blank\">test-trainer</a></strong> to <a href=\"https://wandb.ai/ptah23/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3559234469544654, metrics={'train_runtime': 191.7394, 'train_samples_per_second': 57.39, 'train_steps_per_second': 7.182, 'total_flos': 406183858377360.0, 'train_loss': 0.3559234469544654, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342aff9-8bc0-4982-b482-922766318809",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b214f6d0-b873-4b9d-8683-ce4cc36dabc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e585cd-c392-406b-ba4b-9161b8cbfc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c17f9d-e7c5-4d11-9f10-4be2949fb2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522a2424-168c-401e-9be9-7359dd7c3c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a0b8a854174d5fa42231b0dbb2a3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8553921568627451, 'f1': 0.8998302207130731}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b4dd08-2e19-4344-9a9c-4dff375ef051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31082a1e-bf70-4bb8-8a85-ddc59a6499f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25f0269-8c6d-42b8-a5e9-16de72eda0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363334</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.878893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>0.460163</td>\n",
       "      <td>0.870098</td>\n",
       "      <td>0.909402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.579010</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.914089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.34233617955736256, metrics={'train_runtime': 115.4451, 'train_samples_per_second': 95.318, 'train_steps_per_second': 11.928, 'total_flos': 406183858377360.0, 'train_loss': 0.34233617955736256, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e36b21-60e7-4269-825e-31f805d5bc08",
   "metadata": {},
   "source": [
    "## SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb738f26-d561-45d3-83b3-aefc6cc95d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd28b24fea142acb8b0b23a54a510ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64956abb845c4acf962d0bd6830c5f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "sst2_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d6e47d-5e8b-4640-890b-05d0a8057e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sst2(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "506e5b79-78c1-4988-87d7-f8f252a746f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb53c89e514445d97fa42755064b6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71af4b0d1d7d446bbf9051911aaf7157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839e30e7ea5e4b6da25d2d118be6186b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sst2_datasets = sst2_datasets.map(tokenize_sst2, batched=True)\n",
    "tokenized_sst2_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50981f3e-acba-4933-a037-702e8a67baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_sst2(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94416f50-099c-4abe-b6ee-74250676afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer-sst2\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_sst2_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_sst2_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_sst2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d06b25b-ec9c-4871-9af4-aebe4ba666f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25257\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 26:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.340811</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.446788</td>\n",
       "      <td>0.894495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.364565</td>\n",
       "      <td>0.916284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer-sst2/checkpoint-500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-1000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-1500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-1500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-2000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-2000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-2500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-2500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-3000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-3000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-3500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-3500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-4000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-4000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-4500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-4500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-5000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-5000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-5500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-5500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-6000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-6000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-6500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-6500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-7000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-7000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-7500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-7500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-8000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-8000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-8500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-8500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-9000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-9000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-9500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-9500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-10000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-10000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-10500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-10500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-11000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-11000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-11500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-11500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-12000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-12000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-12500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-12500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-13000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-13000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-13500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-13500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-14000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-14000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-14500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-14500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-15000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-15000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-15500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-15500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-16000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-16000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-16500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-16500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-17000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-17000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-17500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-17500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-18000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-18000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-18500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-18500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-19000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-19000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-19500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-19500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-20000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-20000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-20500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-20500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-21000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-21000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-21500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-21500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-22000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-22000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-22500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-22500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-23000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-23000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-23500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-23500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-24000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-24000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-24500\n",
      "Configuration saved in test-trainer-sst2/checkpoint-24500/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer-sst2/checkpoint-25000\n",
      "Configuration saved in test-trainer-sst2/checkpoint-25000/config.json\n",
      "Model weights saved in test-trainer-sst2/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer-sst2/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer-sst2/checkpoint-25000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25257, training_loss=0.20751524524724213, metrics={'train_runtime': 1592.2249, 'train_samples_per_second': 126.896, 'train_steps_per_second': 15.863, 'total_flos': 3088292734504560.0, 'train_loss': 0.20751524524724213, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8621cb-8a5d-4b96-ab8c-3ca04458d8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
