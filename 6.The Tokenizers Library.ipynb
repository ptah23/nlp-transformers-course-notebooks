{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f2d1dc-4982-430a-85f7-feefe9889a2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b22fc-a7e2-4c90-a571-54e214ce54b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Assembling a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612189c7-dd07-4a93-a1e7-71e854011595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486224ad-f63f-45ef-8c59-11b5d4772988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf69bd396284c33ba89a41086340969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45191e47-1ecb-4df3-9c01-b6f8197bdc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b885fe168d4b0a9f40f6c920e4f22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fd7b90f27d43e3a610aa31fc2d2532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a4fee6d9a04323a4535c2b4d790636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9e57e4116040ee924ba3929809c467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5306a99ecf4449ada93953500b89643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1403f1e8ad42fdb6ea6a183fe5a21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc62e113a541c891162887b4fd9eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cac8b3f61744b9ebabe39db1c7ea5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06073dcf54d4d029564c047eb6a0f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ff577a3aa4afda8c089a20edd8f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c26f977-39c0-486c-9ae4-5d029f2bc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def oauth_token_create(self, data, **kwargs):\n",
      "        \"https://developer.zendesk.com/rest_api/docs/core/oauth_tokens#create-token\"\n",
      "        api_path = \"/api/v2/oauth/tokens.json\"\n",
      "        return self.call(api_path, method=\"POST\", data=data, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38e349-4d7f-447c-a52c-457c53c98490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training a new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68080c5a-a9b2-4990-a27e-a44004496039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dataset into iterator of list of texts\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i:i+1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c3126d-50fa-4e53-a4c1-d6f4a324d610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a68ea58c040189e69ea564b0abde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0768e592a714aa4a3b249732d0ebd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4392eac77fed4543b5d62fa30d474a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d23e6ae1e90448db7eb2f793de514d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a345c80-4932-4c98-bf8c-a9ed9c95308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e53e88d-a670-4939-9048-d6b4b82a2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629f30af-ab6c-489a-a46c-3818258da9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27939887-78b8-4c2a-a518-5327568cc028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf885cc-f84c-46da-9010-202e077e9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "# double indentation token and special python words are now one token each. also camelcase is split\n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b6b7d-c2fe-46a9-be54-389c59d25141",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61c3eca-d992-4c0d-905e-f8bde15263f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b077d7-97ed-4902-9420-2516ba9e721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ptah23/code-search-net-tokenizer/commit/f82a174cdb01c8c07dfa001cce9ca937173005be', commit_message='Upload tokenizer', commit_description='', oid='f82a174cdb01c8c07dfa001cce9ca937173005be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb52745-ee4f-470e-95e4-464db3498517",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Fast tokenizers' special powers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01046e-84e2-4b23-af19-abfabf5d8641",
   "metadata": {},
   "source": [
    "## Batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0434eb6-a386-4524-912c-2aa5f0fcf5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37654769ab5f474eb42696b6c2c0e704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cb491eaaed4ca0a1766ab67184bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acad451e0ed4564b17496ad3c1c486c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce0624abbce4239a27e87d43d7bf7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcb9905-197f-44cf-ade8-768a10663402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e35d457-ff3a-4e93-9870-f4b9f02fc342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b806f8a9-1de8-4100-9053-64317f6d1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b58563a-7898-4de3-a334-54c9545abc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa92a7d6-2545-4523-b74c-3f05a300f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, None]\n",
      "['[CLS]', '81', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "eighty_ones = tokenizer(\"81s\")\n",
    "print(eighty_ones.word_ids())\n",
    "print(eighty_ones.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e9baf7-5255-4a74-9234-2aa7336c0a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dbd5921bd3459f8fb504d63849f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15dac3af45d43aca334f45f5a5a7ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d21b1cb8f4ac28568c704421b160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e40832266534077a0bc2e4c31047b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, None]\n",
      "['<s>', '81', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "eighty_ones_roberta = roberta_tokenizer(\"81s\")\n",
    "print(eighty_ones_roberta.word_ids())\n",
    "print(eighty_ones_roberta.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90c8f12-f7e7-4cd9-a4a3-3f99d4035553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dd51c4-a503-4350-a4fb-3cf65d6dde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n",
      "['[CLS]', 'Imagine', 'Peter', 'is', 'standing', 'in', 'a', 'valley', 'and', 'you', 'want', 'to', 'find', 'the', 'lowest', 'point', '.', 'You', 'can', \"'\", 't', 'see', 'the', 'entire', 'valley', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer(\"Imagine Peter is standing in a valley and you want to find the lowest point. You can't see the entire valley\")\n",
    "print(encoded_sentences.word_ids())\n",
    "print(encoded_sentences.sequence_ids())\n",
    "print(encoded_sentences.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e4a7b-8599-405a-a5ea-3dadc6584ee4",
   "metadata": {},
   "source": [
    "## Inside the `token-classification` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d094e78-ed4d-41c3-877a-3e02330d2acb",
   "metadata": {},
   "source": [
    "### Getting the base results with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a20edf-ac54-4652-912a-6ee52dba0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.9959072, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb08774-29eb-4e42-ba62-6cab9299ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\" , aggregation_strategy=\"simple\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b9809-6d58-4ab3-ba9d-5f36d9ef4f9d",
   "metadata": {},
   "source": [
    "### From inputs to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3e6e9f7-826d-4309-829d-7cd7eca6c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Peter Gelderbloem and I work at Building a Food Forest Scotland in Glasgow.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbb5942b-c5b1-4863-926f-bd2618ac6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n",
      "torch.Size([1, 24, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e75ff723-c665-42b8-9471-9358ab45a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'Peter', 'G', '##eld', '##er', '##b', '##lo', '##em', 'and', 'I', 'work', 'at', 'Building', 'a', 'Food', 'Forest', 'Scotland', 'in', 'Glasgow', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b51974f-7076-4e7a-b5b9-fdaaa272c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d2798be-5a67-40a4-a0d8-072b21e49087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69045e24-f150-4051-82b6-5b52e2575140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.999496340751648, 'word': 'Peter'}, {'entity': 'I-PER', 'score': 0.9996267557144165, 'word': 'G'}, {'entity': 'I-PER', 'score': 0.9973791837692261, 'word': '##eld'}, {'entity': 'I-PER', 'score': 0.9930210709571838, 'word': '##er'}, {'entity': 'I-PER', 'score': 0.995133101940155, 'word': '##b'}, {'entity': 'I-PER', 'score': 0.9220014214515686, 'word': '##lo'}, {'entity': 'I-PER', 'score': 0.9163966774940491, 'word': '##em'}, {'entity': 'I-ORG', 'score': 0.9970241189002991, 'word': 'Building'}, {'entity': 'I-ORG', 'score': 0.9891746044158936, 'word': 'a'}, {'entity': 'I-ORG', 'score': 0.9970507621765137, 'word': 'Food'}, {'entity': 'I-ORG', 'score': 0.9965258240699768, 'word': 'Forest'}, {'entity': 'I-ORG', 'score': 0.9980871677398682, 'word': 'Scotland'}, {'entity': 'I-LOC', 'score': 0.9972265362739563, 'word': 'Glasgow'}]\n"
     ]
    }
   ],
   "source": [
    "results =[]\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append({\"entity\":label, \"score\":probabilities[idx][pred], \"word\":tokens[idx]})\n",
    "print(results)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dedd4dd9-d050-4d62-93f5-2a75aa9ff58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 16), (17, 18), (18, 21), (21, 23), (23, 24), (24, 26), (26, 28), (29, 32), (33, 34), (35, 39), (40, 42), (43, 51), (52, 53), (54, 58), (59, 65), (66, 74), (75, 77), (78, 85), (85, 86), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "print(inputs_with_offsets[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da64da64-1cf9-4a5b-97c2-2f2fb9b8976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peter'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[11:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48785293-9b0c-4114-ba4b-51fd5d70226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.999496340751648, 'word': 'Peter', 'start': 11, 'end': 16}, {'entity': 'I-PER', 'score': 0.9996267557144165, 'word': 'G', 'start': 17, 'end': 18}, {'entity': 'I-PER', 'score': 0.9973791837692261, 'word': '##eld', 'start': 18, 'end': 21}, {'entity': 'I-PER', 'score': 0.9930210709571838, 'word': '##er', 'start': 21, 'end': 23}, {'entity': 'I-PER', 'score': 0.995133101940155, 'word': '##b', 'start': 23, 'end': 24}, {'entity': 'I-PER', 'score': 0.9220014214515686, 'word': '##lo', 'start': 24, 'end': 26}, {'entity': 'I-PER', 'score': 0.9163966774940491, 'word': '##em', 'start': 26, 'end': 28}, {'entity': 'I-ORG', 'score': 0.9970241189002991, 'word': 'Building', 'start': 43, 'end': 51}, {'entity': 'I-ORG', 'score': 0.9891746044158936, 'word': 'a', 'start': 52, 'end': 53}, {'entity': 'I-ORG', 'score': 0.9970507621765137, 'word': 'Food', 'start': 54, 'end': 58}, {'entity': 'I-ORG', 'score': 0.9965258240699768, 'word': 'Forest', 'start': 59, 'end': 65}, {'entity': 'I-ORG', 'score': 0.9980871677398682, 'word': 'Scotland', 'start': 66, 'end': 74}, {'entity': 'I-LOC', 'score': 0.9972265362739563, 'word': 'Glasgow', 'start': 78, 'end': 85}]\n"
     ]
    }
   ],
   "source": [
    "results =[]\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append({\"entity\":label, \"score\":probabilities[idx][pred], \"word\":tokens[idx], \"start\": start, \"end\": end})\n",
    "print(results)\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132171f-c5ab-4d44-9afd-5140bbe7fc15",
   "metadata": {},
   "source": [
    "### Grouping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b05a2b15-b0b5-4e8a-b9a8-6e8b9df7bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.974722078868321, 'word': 'Peter Gelderbloem', 'start': 11, 'end': 28}, {'entity_group': 'ORG', 'score': 0.9955724954605103, 'word': 'Building a Food Forest Scotland', 'start': 43, 'end': 74}, {'entity_group': 'LOC', 'score': 0.9972265362739563, 'word': 'Glasgow', 'start': 78, 'end': 85}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1575db-63b5-4242-a370-bad9d56d1771",
   "metadata": {},
   "source": [
    "# Fast Tokenizers in the QA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586b690-551d-4acf-bb89-0fbc7b6b6461",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Using the `question-answering` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bb1f30-01af-4117-8d8f-530618b23f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bc20897293432690c66afa9949580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c68fabb54147e88395266954be807e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fd546d43834799a2912ab9862215ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e29fbed6cc48638a0917efbf988f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98530681e73f4e2b99760f74d4dc727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802603125572205,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99da9f4e-63d4-4b2d-bfb4-e655f3769472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# long context\n",
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5f1c3-153a-4df7-a1ee-993878b1194b",
   "metadata": {},
   "source": [
    "## Using a model for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0246c950-35f1-4719-9864-e1a0f3227a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0d40e9-e7de-459a-8559-ca71972e7117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1129fde3-335c-4d62-9efa-57d671f73680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "import torch\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76b831a-4f7a-4431-8a9f-b53c286f9e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "mask = [i!=1 for i in sequence_ids]\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977f5e30-01ca-4602-a3bd-e2f0e0c87b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9dd2fa-da4b-4eeb-9812-4a51e853531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5816bdf8-8586-4e9b-bfc1-455d18601103",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37bf9b0-c787-49e0-8190-2ea316cb3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine probabilities\n",
    "scores = start_probabilities[:, None] * end_probabilities[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30a59a2-5f00-4738-b615-48b5a37c325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask values where start_index > end_index\n",
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e82b59-1ad7-4cc7-b1fc-8491999cfd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get max\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abfbe06b-8df8-4eec-9f84-fb0f333d5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093b355e-bf9f-4a87-854f-76252f9fff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643ac90-1ca5-4dfa-92bc-9d370d2d803c",
   "metadata": {},
   "source": [
    "## Handling long contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46818404-aa42-419d-89e8-b8ae144a3e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "# > 384 max length\n",
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dbe0076-d2d9-4cb1-8d78-94477903184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
     ]
    }
   ],
   "source": [
    "# truncate context with answer at end\n",
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "536e5dc5-799d-4bd8-87f1-5a9bce6595b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# split context instead with overlap wo we don't split the answer\n",
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "832860c3-3189-40f2-bbe5-20d100d69509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0636ad78-ae2b-4e48-9c02-70bdf4bf21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# all from the one sentence we passed in\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3273c9a-0492-4e74-9077-71a62d47db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a1b9645-a648-47ba-9141-81a1fd7d5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89df10ae-bba0-4ec1-a09d-2196156421de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "# pop what is not used\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b52bc2a-083e-4ed1-bd08-0966733bde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "# context split in 2 so 2 sets on start and end logits\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61672a93-b3be-4cd4-a33f-5ca9eaeefc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a08127d7-e36f-4420-8e9b-f5df00eac4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e00d1a05-53af-4c28-93d7-6fbde6d20988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.3386709690093994), (173, 184, 0.9714868664741516)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11497f5-bb90-4f0e-bb63-22d816bf183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386709690093994}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e727864-e2bf-4ec5-86df-4cbfdd9bc8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
