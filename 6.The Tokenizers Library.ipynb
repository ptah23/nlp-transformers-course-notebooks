{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f2d1dc-4982-430a-85f7-feefe9889a2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b22fc-a7e2-4c90-a571-54e214ce54b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Assembling a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612189c7-dd07-4a93-a1e7-71e854011595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486224ad-f63f-45ef-8c59-11b5d4772988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf69bd396284c33ba89a41086340969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45191e47-1ecb-4df3-9c01-b6f8197bdc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b885fe168d4b0a9f40f6c920e4f22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fd7b90f27d43e3a610aa31fc2d2532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a4fee6d9a04323a4535c2b4d790636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9e57e4116040ee924ba3929809c467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5306a99ecf4449ada93953500b89643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1403f1e8ad42fdb6ea6a183fe5a21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc62e113a541c891162887b4fd9eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cac8b3f61744b9ebabe39db1c7ea5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06073dcf54d4d029564c047eb6a0f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ff577a3aa4afda8c089a20edd8f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c26f977-39c0-486c-9ae4-5d029f2bc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def oauth_token_create(self, data, **kwargs):\n",
      "        \"https://developer.zendesk.com/rest_api/docs/core/oauth_tokens#create-token\"\n",
      "        api_path = \"/api/v2/oauth/tokens.json\"\n",
      "        return self.call(api_path, method=\"POST\", data=data, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38e349-4d7f-447c-a52c-457c53c98490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training a new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68080c5a-a9b2-4990-a27e-a44004496039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dataset into iterator of list of texts\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i:i+1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c3126d-50fa-4e53-a4c1-d6f4a324d610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a68ea58c040189e69ea564b0abde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0768e592a714aa4a3b249732d0ebd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4392eac77fed4543b5d62fa30d474a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d23e6ae1e90448db7eb2f793de514d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a345c80-4932-4c98-bf8c-a9ed9c95308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e53e88d-a670-4939-9048-d6b4b82a2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629f30af-ab6c-489a-a46c-3818258da9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27939887-78b8-4c2a-a518-5327568cc028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf885cc-f84c-46da-9010-202e077e9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n"
     ]
    }
   ],
   "source": [
    "# double indentation token and special python words are now one token each. also camelcase is split\n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b6b7d-c2fe-46a9-be54-389c59d25141",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61c3eca-d992-4c0d-905e-f8bde15263f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b077d7-97ed-4902-9420-2516ba9e721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ptah23/code-search-net-tokenizer/commit/f82a174cdb01c8c07dfa001cce9ca937173005be', commit_message='Upload tokenizer', commit_description='', oid='f82a174cdb01c8c07dfa001cce9ca937173005be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb52745-ee4f-470e-95e4-464db3498517",
   "metadata": {},
   "source": [
    "# Fast tokenizers' special powers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01046e-84e2-4b23-af19-abfabf5d8641",
   "metadata": {},
   "source": [
    "## Batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0434eb6-a386-4524-912c-2aa5f0fcf5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37654769ab5f474eb42696b6c2c0e704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cb491eaaed4ca0a1766ab67184bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acad451e0ed4564b17496ad3c1c486c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce0624abbce4239a27e87d43d7bf7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcb9905-197f-44cf-ade8-768a10663402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e35d457-ff3a-4e93-9870-f4b9f02fc342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b806f8a9-1de8-4100-9053-64317f6d1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b58563a-7898-4de3-a334-54c9545abc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa92a7d6-2545-4523-b74c-3f05a300f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, None]\n",
      "['[CLS]', '81', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "eighty_ones = tokenizer(\"81s\")\n",
    "print(eighty_ones.word_ids())\n",
    "print(eighty_ones.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e9baf7-5255-4a74-9234-2aa7336c0a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dbd5921bd3459f8fb504d63849f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15dac3af45d43aca334f45f5a5a7ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d21b1cb8f4ac28568c704421b160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e40832266534077a0bc2e4c31047b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, None]\n",
      "['<s>', '81', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "eighty_ones_roberta = roberta_tokenizer(\"81s\")\n",
    "print(eighty_ones_roberta.word_ids())\n",
    "print(eighty_ones_roberta.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90c8f12-f7e7-4cd9-a4a3-3f99d4035553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dd51c4-a503-4350-a4fb-3cf65d6dde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n",
      "['[CLS]', 'Imagine', 'Peter', 'is', 'standing', 'in', 'a', 'valley', 'and', 'you', 'want', 'to', 'find', 'the', 'lowest', 'point', '.', 'You', 'can', \"'\", 't', 'see', 'the', 'entire', 'valley', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer(\"Imagine Peter is standing in a valley and you want to find the lowest point. You can't see the entire valley\")\n",
    "print(encoded_sentences.word_ids())\n",
    "print(encoded_sentences.sequence_ids())\n",
    "print(encoded_sentences.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e4a7b-8599-405a-a5ea-3dadc6584ee4",
   "metadata": {},
   "source": [
    "## Inside the `token-classification` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d094e78-ed4d-41c3-877a-3e02330d2acb",
   "metadata": {},
   "source": [
    "### Getting the base results with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a20edf-ac54-4652-912a-6ee52dba0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.9959072, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb08774-29eb-4e42-ba62-6cab9299ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\" , aggregation_strategy=\"simple\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b9809-6d58-4ab3-ba9d-5f36d9ef4f9d",
   "metadata": {},
   "source": [
    "### From inputs to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3e6e9f7-826d-4309-829d-7cd7eca6c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Peter Gelderbloem and I work at Building a Food Forest Scotland in Glasgow.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbb5942b-c5b1-4863-926f-bd2618ac6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n",
      "torch.Size([1, 24, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e75ff723-c665-42b8-9471-9358ab45a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'Peter', 'G', '##eld', '##er', '##b', '##lo', '##em', 'and', 'I', 'work', 'at', 'Building', 'a', 'Food', 'Forest', 'Scotland', 'in', 'Glasgow', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b51974f-7076-4e7a-b5b9-fdaaa272c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d2798be-5a67-40a4-a0d8-072b21e49087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69045e24-f150-4051-82b6-5b52e2575140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.999496340751648, 'word': 'Peter'}, {'entity': 'I-PER', 'score': 0.9996267557144165, 'word': 'G'}, {'entity': 'I-PER', 'score': 0.9973791837692261, 'word': '##eld'}, {'entity': 'I-PER', 'score': 0.9930210709571838, 'word': '##er'}, {'entity': 'I-PER', 'score': 0.995133101940155, 'word': '##b'}, {'entity': 'I-PER', 'score': 0.9220014214515686, 'word': '##lo'}, {'entity': 'I-PER', 'score': 0.9163966774940491, 'word': '##em'}, {'entity': 'I-ORG', 'score': 0.9970241189002991, 'word': 'Building'}, {'entity': 'I-ORG', 'score': 0.9891746044158936, 'word': 'a'}, {'entity': 'I-ORG', 'score': 0.9970507621765137, 'word': 'Food'}, {'entity': 'I-ORG', 'score': 0.9965258240699768, 'word': 'Forest'}, {'entity': 'I-ORG', 'score': 0.9980871677398682, 'word': 'Scotland'}, {'entity': 'I-LOC', 'score': 0.9972265362739563, 'word': 'Glasgow'}]\n"
     ]
    }
   ],
   "source": [
    "results =[]\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append({\"entity\":label, \"score\":probabilities[idx][pred], \"word\":tokens[idx]})\n",
    "print(results)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dedd4dd9-d050-4d62-93f5-2a75aa9ff58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 16), (17, 18), (18, 21), (21, 23), (23, 24), (24, 26), (26, 28), (29, 32), (33, 34), (35, 39), (40, 42), (43, 51), (52, 53), (54, 58), (59, 65), (66, 74), (75, 77), (78, 85), (85, 86), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "print(inputs_with_offsets[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da64da64-1cf9-4a5b-97c2-2f2fb9b8976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peter'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[11:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48785293-9b0c-4114-ba4b-51fd5d70226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.999496340751648, 'word': 'Peter', 'start': 11, 'end': 16}, {'entity': 'I-PER', 'score': 0.9996267557144165, 'word': 'G', 'start': 17, 'end': 18}, {'entity': 'I-PER', 'score': 0.9973791837692261, 'word': '##eld', 'start': 18, 'end': 21}, {'entity': 'I-PER', 'score': 0.9930210709571838, 'word': '##er', 'start': 21, 'end': 23}, {'entity': 'I-PER', 'score': 0.995133101940155, 'word': '##b', 'start': 23, 'end': 24}, {'entity': 'I-PER', 'score': 0.9220014214515686, 'word': '##lo', 'start': 24, 'end': 26}, {'entity': 'I-PER', 'score': 0.9163966774940491, 'word': '##em', 'start': 26, 'end': 28}, {'entity': 'I-ORG', 'score': 0.9970241189002991, 'word': 'Building', 'start': 43, 'end': 51}, {'entity': 'I-ORG', 'score': 0.9891746044158936, 'word': 'a', 'start': 52, 'end': 53}, {'entity': 'I-ORG', 'score': 0.9970507621765137, 'word': 'Food', 'start': 54, 'end': 58}, {'entity': 'I-ORG', 'score': 0.9965258240699768, 'word': 'Forest', 'start': 59, 'end': 65}, {'entity': 'I-ORG', 'score': 0.9980871677398682, 'word': 'Scotland', 'start': 66, 'end': 74}, {'entity': 'I-LOC', 'score': 0.9972265362739563, 'word': 'Glasgow', 'start': 78, 'end': 85}]\n"
     ]
    }
   ],
   "source": [
    "results =[]\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append({\"entity\":label, \"score\":probabilities[idx][pred], \"word\":tokens[idx], \"start\": start, \"end\": end})\n",
    "print(results)\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132171f-c5ab-4d44-9afd-5140bbe7fc15",
   "metadata": {},
   "source": [
    "### Grouping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b05a2b15-b0b5-4e8a-b9a8-6e8b9df7bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.974722078868321, 'word': 'Peter Gelderbloem', 'start': 11, 'end': 28}, {'entity_group': 'ORG', 'score': 0.9955724954605103, 'word': 'Building a Food Forest Scotland', 'start': 43, 'end': 74}, {'entity_group': 'LOC', 'score': 0.9972265362739563, 'word': 'Glasgow', 'start': 78, 'end': 85}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440cca5-7693-42c2-9fa3-a26e2ab14f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
